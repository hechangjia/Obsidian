---
title: "残差神经网络 - 维基百科，自由的百科全书"
date: "2025-02-25T19:37:21+08:00"
author:
  - "[[维基媒体项目贡献者]]"
tags:
  - "clippings"
category: "残差神经网络"
rating: "8"
published: 2024-01-27
description:
source: "https://zh.wikipedia.org/wiki/%E6%AE%8B%E5%B7%AE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"
---
![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/ResBlock.png/220px-ResBlock.png)

在残差神经网络中的一个残差块里，残差连接跳过了两个网络层。

**残差神经网络**（**Residual Neural Network**，简称**ResNet**）[^resnet-1]属于深度学习模型的一种，其核心在于让网络的每一层不直接学习预期输出，而是学习与输入之间的残差关系。这种网络通过添加“跳跃连接”，即跳过某些网络层的连接来实现身份映射，再与网络层的输出相加合并。其运作机制与[高速神经网络](https://zh.wikipedia.org/w/index.php?title=%E9%AB%98%E9%80%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&action=edit&redlink=1 "高速神经网络（页面不存在）")类似，通过极大的正偏置权重来打开“门控”。[^highway2015may-2] 这一设计使得拥有几十上百层的深度学习模型可以更易于训练，增加模型深度时还能保持甚至提高准确度。所谓的“残差连接”即“直连跳过”，这一概念也被应用于1997年的[长短期记忆](https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6 "长短期记忆")模型LSTM、[^lstm1997-3] [Transformer模型](https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B "Transformer模型")（比如[BERT](https://zh.wikipedia.org/wiki/BERT "BERT")和[GPT](https://zh.wikipedia.org/wiki/%E5%9F%BA%E4%BA%8E%E8%BD%AC%E6%8D%A2%E5%99%A8%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B "基于转换器的生成式预训练模型")系列，[ChatGPT](https://zh.wikipedia.org/wiki/ChatGPT "ChatGPT")等）、[AlphaGo Zero](https://zh.wikipedia.org/wiki/AlphaGo_Zero "AlphaGo Zero")、[AlphaStar](https://zh.wikipedia.org/w/index.php?title=AlphaStar&action=edit&redlink=1 "AlphaStar（页面不存在）")以及[AlphaFold](https://zh.wikipedia.org/wiki/AlphaFold "AlphaFold")等。

残差神经网络由[何恺明](https://zh.wikipedia.org/wiki/%E4%BD%95%E6%81%BA%E6%98%8E "何恺明")、张祥雨、任少卿和孙剑开发，这一成果在2015年的[ImageNet](https://zh.wikipedia.org/wiki/ImageNet "ImageNet")大规模视觉识别挑战赛中夺冠。[^imagenet-4][^ilsvrc2015-5]

2012年，针对[ImageNet](https://zh.wikipedia.org/wiki/ImageNet "ImageNet")竞赛开发的[AlexNet](https://zh.wikipedia.org/wiki/AlexNet "AlexNet")模型是一个包含8层的[卷积神经网络](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "卷积神经网络")。到了2014年，[牛津大学](https://zh.wikipedia.org/wiki/%E7%89%9B%E6%B4%A5%E5%A4%A7%E5%AD%A6 "牛津大学")的视觉几何组（VGG）通过叠加3x3卷积层将网络深度增加到了19层。[^vggnet-6] 但是，层级的增加却导致训练精度的迅速下降，[^prelu-7] 这种现象被称为“性能退化”问题。[^resnet-1]

理论上，如果一个更深的网络仅仅是通过在一个较浅网络的基础上增加额外层来构建的，那么这个更深的网络不应该比其较浅的网络有更高的训练损失。[^resnet-1] 如果这些额外层具有身份映射的能力，那么更深的网络应该能够实现与其较浅网络相同的功能。但这里存在一个假设，即优化器不能有效地将这些参数化的网络层调整为身份映射。

在多层神经网络模型里，设想一个包含若干层的子网络。这个子网络的函数用  ${\textstyle H(x)}$  来表示，其中  ${\textstyle x}$  是子网络的输入。残差学习是通过重新设定这个子网络的参数，让参数层表达一个残差函数  ${\textstyle F(x):=H(x)-x}$  。因此，这个子网络的输出  ${\textstyle y}$  可以表示为：

${\displaystyle {\begin{aligned}y&=F(x)+x\end{aligned}}}$ 

这一原理同样适用于1997年提出的[长短期记忆](https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6 "长短期记忆")LSTM单元，[^lstm1997-3] 在[随时间反向传播](https://zh.wikipedia.org/w/index.php?title=%E9%9A%8F%E6%97%B6%E9%97%B4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&action=edit&redlink=1 "随时间反向传播（页面不存在）")里计算  ${\textstyle y_{t+1}=F(x_{t})+x_{t}}$  ，简化为  ${\textstyle y=F(x)+x}$  。

函数  ${\textstyle F(x)}$  常通过矩阵乘法实现，并结合[激活函数](https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0 "激活函数")以及规范化操作（如[批量规范化](https://zh.wikipedia.org/w/index.php?title=%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96&action=edit&redlink=1 "批量规范化（页面不存在）")或层规范化）。

这类子网络被称作“残差块”。[^resnet-1] 通过叠加这样的残差块，形成深度残差网络。

在"  ${\textstyle y=F(x)+x}$  "公式中的"  ${\textstyle +\ x}$  "操作是通过一个相当于恒等映射的跳跃连接来完成，它将残差块的输入直接与输出连接。在随后的研究中，这种连接常被称作“残差连接”。[^inceptionv4-8]

身份映射的引入有利于信号在前向传播路径和反向传播路径中的传递。[^resnetv2-9]

如果第  ${\textstyle \ell }$  个残差块的输出是第  ${\textstyle (\ell +1)}$  个残差块的输入（这里假设块与块之间没有激活函数），可以得到：[^resnetv2-9]

${\displaystyle {\begin{aligned}x_{\ell +1}&=F(x_{\ell })+x_{\ell }\end{aligned}}}$ 

若递归应用此公式，例如，  ${\displaystyle {\begin{aligned}x_{\ell +2}=F(x_{\ell +1})+x_{\ell +1}=F(x_{\ell +1})+F(x_{\ell })+x_{\ell }\end{aligned}}}$  ，可以推导出：

${\displaystyle {\begin{aligned}x_{L}&=x_{\ell }+\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}$ 

这里  ${\textstyle L}$  表示任意后续残差块的索引（比如处于最末尾的块），  ${\textstyle \ell }$  代表任意靠前的块对应的索引。该公式说明了总有一个信号能够直接从浅层块  ${\textstyle \ell }$  传递到深层块  ${\textstyle L}$  。

残差学习的公式还在一定程度上缓解了[梯度消失问题](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98 "梯度消失问题")。然而，梯度消失并不是导致性能退化问题的根源，因为通过引入规范化层（如批量规范化）可在一定程度上解决此问题。根据上面的前向传播过程，对  ${\textstyle x_{\ell }}$  进行求导，可以得到：[^resnetv2-9]

${\displaystyle {\begin{aligned}{\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial x_{L}}{\partial x_{\ell }}}\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}\left(1+{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\right)\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}+{\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}$ 

这里  ${\textstyle {\mathcal {E}}}$ 是最小化损失函数。以上表明，浅层的梯度计算  ${\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}}$  总会直接加上一个项  ${\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}}$  。因此，由于额外项  ${\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}}$  的存在，即使  ${\textstyle F(x_{i})}$  的梯度很小，总梯度  ${\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}}$  也不会消失。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/ResBlockVariants.png/220px-ResBlockVariants.png)

两种类型的卷积残差块。左侧是基本块，它由两个3x3卷积层组成。右侧是瓶颈块，该块先通过一个1x1卷积层进行降维，接着是一个3x3卷积层，最后再通过一个1x1卷积层恢复原来的维度。

基本残差块是原始ResNet研究中最简单的部分。[^resnet-1] 它包括两个串行的3x3[卷积层](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "卷积神经网络")以及一个残差连接。这两层的输入输出尺寸保持一致。

瓶颈残差块包含三个串联的卷积层和一个残差连接。[^resnet-1] 该块的第一层是1x1卷积，用于降维，比如降至输入维度的1/4；第二层是3x3卷积；最后一层是另一个1x1卷积，用于恢复维度。ResNet-50、ResNet-101和ResNet-152模型都基于瓶颈块构建。[^resnet-1]

预激活残差块[^resnetv2-9]在应用残差函数  ${\textstyle F}$  之前，先使用激活函数，如非线性和规范化的处理。预激活残差块的计算可以表述为：

${\displaystyle {\begin{aligned}x_{\ell +1}&=F(\phi (x_{\ell }))+x_{\ell }\end{aligned}}}$ 

这里的  ${\textstyle \phi }$  可以是如[线性整流函数](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0 "线性整流函数")等任意非线性激活或归一化操作。这种设计减少了残差块间非恒等映射的数量，被用于训练200层到1000多层的模型。[^resnetv2-9]

从[GPT-2](https://zh.wikipedia.org/wiki/GPT-2 "GPT-2")开始，[Transformer](https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B "Transformer模型")块常被用于预激活块，这在Transformer模型的相关文献中被称为“预规范化”。[^gpt2paper-10]

![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Full_GPT_architecture.svg/220px-Full_GPT_architecture.svg.png)

原始[GPT](https://zh.wikipedia.org/wiki/%E5%9F%BA%E4%BA%8E%E8%BD%AC%E6%8D%A2%E5%99%A8%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B "基于转换器的生成式预训练模型")模型采用的[Transformer](https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B "Transformer模型")架构是由两种类型的残差块构成：一个是多头注意力块，另一个是前馈的[多层感知器](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8 "多层感知器")（MLP）块。这种设计通过结合两个功能强大的残差块，使得Transformer能够高效地处理数据并学习复杂的特征，其中每个残差块都利用残差连接来促使信号在网络深层之间的流动以及更有效的进行梯度回传，克服了深度模型训练过程中遇到的梯度消失等问题。

Transformer块是由两个残差块组成，每个残差块都设有一个残差连接。

第一个残差块为多头注意力块，使用了自注意力运算，随后连接一个线性映射层。第二个残差块是一个前馈式的[多层感知器](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8 "多层感知器")（MLP）块，这个块在某种程度上像是一个“反向”的瓶颈块，它通过一个线性映射层（在卷积神经网络中相当于1x1的卷积）来扩大维度，然后通过另一个线性映射层来减少维度。

一个Transformer块包含了四层线性映射。[GPT-3](https://zh.wikipedia.org/wiki/GPT-3 "GPT-3")模型拥有96个这样的Transformer块（在Transformer领域的文献中，通常将一个Transformer块称作一个“Transformer层”）。因此，该模型包含了大约400层的映射层，包括Transformer块内的96x4层，以及一些额外的层用于输入嵌入和输出预测。

若没有残差连接，训练网络深度极高的Transformer模型将无法获取成功。[^lose_rank-11]

1961年，在[弗兰克·罗森布拉特](https://zh.wikipedia.org/w/index.php?title=%E5%BC%97%E5%85%B0%E5%85%8B%C2%B7%E7%BD%97%E6%A3%AE%E5%B8%83%E6%8B%89%E7%89%B9&action=edit&redlink=1 "弗兰克·罗森布拉特（页面不存在）")出版的书籍中介绍了一个含有跳跃连接的三层[多层感知器](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8 "多层感知器")（MLP）模型（详见第15章，第313页[^mlpbook-12]）。这种模型被称作“交叉耦合系统”，其跳跃连接实际上是交叉耦合连接的一种形式。[^mlpbook-12]

在1994年[^massbook-13]和1996年[^prnnbook-14]出版的书籍中提出了在前馈MLP模型中使用的“跳层”连接：“MLP一般允许存在多个隐藏层，并且支持从输入直接到输出的‘跳层’连接”（详见第261页[^massbook-13]，第144页[^prnnbook-14]），“...这使得非线性单元能够调整线性函数形式”（详见第262页[^massbook-13]）。这种说法其实已经暗示了非线性MLP的表现就像是在一个线性函数上加上了一个残差函数。

[塞普·霍赫赖特](https://zh.wikipedia.org/w/index.php?title=%E5%A1%9E%E6%99%AE%C2%B7%E9%9C%8D%E8%B5%AB%E8%B5%96%E7%89%B9&action=edit&redlink=1 "塞普·霍赫赖特（页面不存在）")在1991年分析了[梯度消失问题](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98 "梯度消失问题")，并认为这是[深度学习](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0 "深度学习")效果不佳的原因。[^hochreiter1991-15] 为解决这一问题，[长短期记忆](https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6 "长短期记忆")（LSTM）网络[^lstm1997-3]在每个LSTM单元中引入了权重为1.0的跳跃或残差连接，计算公式为  ${\textstyle y_{t+1}=F(x_{t})+x_{t}}$  。在[随时间反向传播](https://zh.wikipedia.org/w/index.php?title=%E9%9A%8F%E6%97%B6%E9%97%B4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&action=edit&redlink=1 "随时间反向传播（页面不存在）")中，这个公式转化为前馈神经网络中的残差公式  ${\textstyle y=F(x)+x}$  ，从而实现了深层[循环神经网络](https://zh.wikipedia.org/wiki/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "循环神经网络")的有效训练。2000年[^lstm2000-16]发布的一种LSTM版本通过“遗忘门”来调节连接，这些门的权重不再固定为1.0，而是可以学习的。在实际实验中，遗忘门通过正偏置权重进行初始化，从而解决梯度消失问题。[^lstm2000-16]

2015年，[高速神经网络](https://zh.wikipedia.org/w/index.php?title=%E9%AB%98%E9%80%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&action=edit&redlink=1 "高速神经网络（页面不存在）")[^highway2015may-2][^highway2015july-17]将上述原理应用于[前馈神经网络](https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "前馈神经网络")，被媒体报道称为“首个具有数百层深度的[前馈神经网络](https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "前馈神经网络")”。[^highwayblog-18] 原有的高速神经网络论文[^highway2015may-2]不仅提出了非常深的前馈网络的基本架构，还展示了20层、50层和100层网络的实验结果，并提及正在进行的深达900层的实验。50层或100层的网络相较于它们常用的神经网络有更低的训练误差，但与20层的训练结果相比，误差并无降低（详见MNIST数据集中的图1[^highway2015may-2]）。在超过19层的网络上，并未有训练精度的提高[^highway2015may-2]。然而，ResNet的研究证明了多于20层以上的训练结果同样有效。[^resnetv2-9] 它指出跳跃连接中的调节可能导致前向和反向传播中信号的消失（详见第3节[^resnetv2-9]）。这也解释了为什么2000年的LSTM[^lstm2000-16]的遗忘门要通过正偏置权重初始化为开启状态：只要门处于开启状态，它就如同1997年的LSTM。同样，高速神经网络如果通过强正偏置权重保持门的开启状态，就表现得如同ResNet一样。现在的神经网络（如[Transformer](https://zh.wikipedia.org/wiki/Transformer%E6%A8%A1%E5%9E%8B "Transformer模型")）中使用的跳跃连接主要就是身份映射。

2016年出现的DenseNets[^19]被设计为一种深层的神经网络，旨在将每一层与其他所有层连接。DenseNets通过使用身份映射作为跳跃连接来实现这一目标。与ResNet不同，DenseNets通过拼接而非加法将层输出与跳跃连接合并。

利用残差网络架构，实现了具有随机深度的神经网络。[^20] 通过随机丢弃一部分网络层，让信号通过跳跃连接进行传播。这种做法也被称为“路径丢弃”。这是训练大型深层模型，如[视觉Transformer](https://zh.wikipedia.org/w/index.php?title=%E8%A7%86%E8%A7%89Transformer&action=edit&redlink=1 "视觉Transformer（页面不存在）")（ViT）的一种十分有效的正则化方法。

虽然最初的残差网络研究并未受[生物学](https://zh.wikipedia.org/wiki/%E7%94%9F%E7%89%A9%E5%AD%A6 "生物学")启发，但后来的研究却发现残差网络与生物学有关。[^liao2016-21][^xiao2018-22]

2023年《[科学](https://zh.wikipedia.org/wiki/%E7%A7%91%E5%AD%A6_\(%E6%9C%9F%E5%88%8A\) "科学 (期刊)")》杂志上发表的一项研究展示了[果蝇](https://zh.wikipedia.org/wiki/%E6%9E%9C%E8%A0%85%E5%B1%AC "果蝇属")[幼虫](https://zh.wikipedia.org/wiki/%E5%B9%BC%E4%BD%93 "幼体")[大脑](https://zh.wikipedia.org/wiki/%E5%A4%A7%E8%84%91 "大脑")的完整神经[连接组](https://zh.wikipedia.org/wiki/%E8%BF%9E%E6%8E%A5%E7%BB%84 "连接组")。[^winding2023-23] 这项研究发现了类似于人工神经网络中如ResNet一样的跳跃连接。

[^resnet-1]: He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian. Deep Residual Learning for Image Recognition. 10 Dec 2015. [arXiv:1512.03385](https://arxiv.org/abs/1512.03385) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) .

[^highway2015may-2]: Srivastava, Rupesh Kumar; Greff, Klaus; Schmidhuber, Jürgen. Highway Networks. 3 May 2015. [arXiv:1505.00387](https://arxiv.org/abs/1505.00387) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.LG](https://arxiv.org/archive/cs.LG)\].

[^lstm1997-3]: Sepp Hochreiter; [Jürgen Schmidhuber](https://zh.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber "Jürgen Schmidhuber"). [Long short-term memory](https://www.researchgate.net/publication/13853244). Neural Computation. 1997, **9** (8): 1735–1780 \[2024-01-27\]. [PMID 9377276](https://www.ncbi.nlm.nih.gov/pubmed/9377276). [S2CID 1915014](https://api.semanticscholar.org/CorpusID:1915014). [doi:10.1162/neco.1997.9.8.1735](https://doi.org/10.1162%2Fneco.1997.9.8.1735). （原始内容[存档](https://web.archive.org/web/20210122144703/https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)于2021-01-22）.

[^imagenet-4]: Deng, Jia; Dong, Wei; Socher, Richard; Li, Li-Jia; Li, Kai; Fei-Fei, Li. [ImageNet: A large-scale hierarchical image database](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=rDfyQnIAAAAJ&citation_for_view=rDfyQnIAAAAJ:qjMakFHDy7sC). CVPR. 2009 \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20190929005334/https://scholar.google.com/citations?view_op=view_citation&hl=en&user=rDfyQnIAAAAJ&citation_for_view=rDfyQnIAAAAJ:qjMakFHDy7sC)于2019-09-29）.

[^ilsvrc2015-5]: [ILSVRC2015 Results](https://image-net.org/challenges/LSVRC/2015/results.php). image-net.org. \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20230929104011/https://www.image-net.org/challenges/LSVRC/2015/results.php)于2023-09-29）.

[^vggnet-6]: Simonyan, Karen; Zisserman, Andrew. Very Deep Convolutional Networks for Large-Scale Image Recognition. 2014. [arXiv:1409.1556](https://arxiv.org/abs/1409.1556) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.CV](https://arxiv.org/archive/cs.CV)\].

[^prelu-7]: He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. 2016. [arXiv:1502.01852](https://arxiv.org/abs/1502.01852) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.CV](https://arxiv.org/archive/cs.CV)\].

[^inceptionv4-8]: Szegedy, Christian; Ioffe, Sergey; Vanhoucke, Vincent; Alemi, Alex. Inception-v4, Inception-ResNet and the impact of residual connections on learning. 2016. [arXiv:1602.07261](https://arxiv.org/abs/1602.07261) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.CV](https://arxiv.org/archive/cs.CV)\].

[^resnetv2-9]: He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian. Identity Mappings in Deep Residual Networks. 2015. [arXiv:1603.05027](https://arxiv.org/abs/1603.05027) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.CV](https://arxiv.org/archive/cs.CV)\].

[^gpt2paper-10]: Radford, Alec; Wu, Jeffrey; Child, Rewon; Luan, David; Amodei, Dario; Sutskever, Ilya. [Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (PDF). 14 February 2019 \[19 December 2020\]. （原始内容[存档](https://web.archive.org/web/20210206183945/https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (PDF)于6 February 2021）.

[^lose_rank-11]: Dong, Yihe; Cordonnier, Jean-Baptiste; Loukas, Andreas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. 2021. [arXiv:2103.03404](https://arxiv.org/abs/2103.03404) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.LG](https://arxiv.org/archive/cs.LG)\].

[^mlpbook-12]: Rosenblatt, Frank. [Principles of neurodynamics. perceptrons and the theory of brain mechanisms](https://safari.ethz.ch/digitaltechnik/spring2018/lib/exe/fetch.php?media=neurodynamics1962rosenblatt.pdf#page=327) (PDF). 1961 \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20230504010603/https://safari.ethz.ch/digitaltechnik/spring2018/lib/exe/fetch.php?media=neurodynamics1962rosenblatt.pdf#page=327) (PDF)于2023-05-04）.

[^massbook-13]: Venables, W. N.; Ripley, Brain D. [Modern Applied Statistics with S-Plus](https://books.google.com/books?id=ayDvAAAAMAAJ). Springer. 1994 \[2024-01-27\]. [ISBN 9783540943501](https://zh.wikipedia.org/wiki/Special:%E7%BD%91%E7%BB%9C%E4%B9%A6%E6%BA%90/9783540943501 "Special:网络书源/9783540943501"). （原始内容[存档](https://web.archive.org/web/20230822220153/https://books.google.com/books?id=ayDvAAAAMAAJ)于2023-08-22）.

[^prnnbook-14]: Ripley, B. D. [Pattern Recognition and Neural Networks](https://www.cambridge.org/core/books/pattern-recognition-and-neural-networks/4E038249C9BAA06C8F4EE6F044D09C5C). Cambridge University Press. 1996 \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20231202035324/https://www.cambridge.org/core/books/pattern-recognition-and-neural-networks/4E038249C9BAA06C8F4EE6F044D09C5C)于2023-12-02）.

[^hochreiter1991-15]: Hochreiter, Sepp. [Untersuchungen zu dynamischen neuronalen Netzen](http://www.bioinf.jku.at/publications/older/3804.pdf) (PDF) (diploma论文). Technical University Munich, Institute of Computer Science, advisor: J. Schmidhuber. 1991 \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20230320225540/http://www.bioinf.jku.at/publications/older/3804.pdf) (PDF)于2023-03-20）.

[^lstm2000-16]: Felix A. Gers; Jürgen Schmidhuber; Fred Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation. 2000, **12** (10): 2451–2471. [CiteSeerX 10.1.1.55.5709](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) . [PMID 11032042](https://www.ncbi.nlm.nih.gov/pubmed/11032042). [S2CID 11598600](https://api.semanticscholar.org/CorpusID:11598600). [doi:10.1162/089976600300015015](https://doi.org/10.1162%2F089976600300015015).

[^highway2015july-17]: Srivastava, Rupesh Kumar; Greff, Klaus; Schmidhuber, Jürgen. Training Very Deep Networks. 22 July 2015. [arXiv:1507.06228](https://arxiv.org/abs/1507.06228) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png)  \[[cs.LG](https://arxiv.org/archive/cs.LG)\].

[^highwayblog-18]: Schmidhuber, Jürgen. [Microsoft Wins ImageNet 2015 through Highway Net (or Feedforward LSTM) without Gates](https://people.idsia.ch/~juergen/microsoft-wins-imagenet-through-feedforward-LSTM-without-gates.html). 2015 \[2024-01-27\]. （原始内容[存档](https://web.archive.org/web/20231127155507/https://people.idsia.ch/~juergen/microsoft-wins-imagenet-through-feedforward-LSTM-without-gates.html)于2023-11-27）.

[^19]: Huang, Gao; Liu, Zhuang; van der Maaten, Laurens; Weinberger, Kilian. Densely Connected Convolutional Networks. 2016. [arXiv:1608.06993](https://arxiv.org/abs/1608.06993) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) .

[^20]: Huang, Gao; Sun, Yu; Liu, Zhuang; Weinberger, Kilian. Deep Networks with Stochastic Depth. 2016. [arXiv:1603.09382](https://arxiv.org/abs/1603.09382) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) .

[^liao2016-21]: Liao, Qianli; Poggio, Tomaso. Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex. 2016. [arXiv:1604.03640](https://arxiv.org/abs/1604.03640) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) .

[^xiao2018-22]: Xiao, Will; Chen, Honglin; Liao, Qianli; Poggio, Tomaso. Biologically-Plausible Learning Algorithms Can Scale to Large Datasets. 2018. [arXiv:1811.03567](https://arxiv.org/abs/1811.03567) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) .

[^winding2023-23]: Winding, Michael; Pedigo, Benjamin; Barnes, Christopher; Patsolic, Heather; Park, Youngser; Kazimiers, Tom; Fushiki, Akira; Andrade, Ingrid; Khandelwal, Avinash; Valdes-Aleman, Javier; Li, Feng; Randel, Nadine; Barsotti, Elizabeth; Correia, Ana; Fetter, Fetter; Hartenstein, Volker; Priebe, Carey; Vogelstein, Joshua; Cardona, Albert; Zlatic, Marta. [The connectome of an insect brain](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7614541). Science. 10 Mar 2023, **379** (6636): eadd9330. [PMC 7614541](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7614541) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) . [PMID 36893230](https://www.ncbi.nlm.nih.gov/pubmed/36893230). [S2CID 254070919](https://api.semanticscholar.org/CorpusID:254070919). [bioRxiv 10.1101/2022.11.28.516756v1](https://doi.org/10.1101%2F2022.11.28.516756v1) ![可免费查阅](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png) . [doi:10.1126/science.add9330](https://doi.org/10.1126%2Fscience.add9330).